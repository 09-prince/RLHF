{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For google colab\n",
    "# !pip install trl\n",
    "# !pip install -U bitsandbytes\n",
    "# !pip install -U accelerate\n",
    "# !pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b35b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# put your huggingface token----\n",
    "login(\"Huggingface_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2753d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import packages\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from peft import LoraConfig\n",
    "from accelerate import Accelerator\n",
    "from dataclasses import dataclass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ddf7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Training arguments\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    model_name_or_path: str = \"meta-llama/Llama-2-7b-hf\"\n",
    "    learning_rate: float = 5e-5\n",
    "    beta: float = 0.1\n",
    "    per_device_train_batch_size: int = 1\n",
    "    per_device_eval_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    gradient_checkpointing: bool = True\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    max_length: int = 1024\n",
    "    max_prompt_length: int = 512\n",
    "    max_steps: int = 100\n",
    "    save_steps: int = 50\n",
    "    eval_steps: int = 50\n",
    "    logging_steps: int = 10\n",
    "    output_dir: str = \"./dpo_llama\"\n",
    "    report_to: str = \"none\"\n",
    "    load_in_4bit: bool = True\n",
    "    model_dtype: str = \"float16\"\n",
    "    seed: int = 42\n",
    "\n",
    "args = ScriptArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a795c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load model\n",
    "torch_dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float\": torch.float}[args.model_dtype]\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    torch_dtype=torch_dtype,\n",
    "    load_in_4bit=args.load_in_4bit,\n",
    "    device_map={\"\": Accelerator().local_process_index},\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load and preprocess dataset (Anthropic hh-rlhf)\n",
    "def split_prompt_response(example):\n",
    "    # Assumes format like: \"Human: ...\\n\\nAssistant: ...\"\n",
    "    sep = \"\\n\\nAssistant:\"\n",
    "    if sep in example[\"chosen\"] and sep in example[\"rejected\"]:\n",
    "        prompt = example[\"chosen\"].split(sep)[0] + sep\n",
    "        chosen = example[\"chosen\"].split(sep)[1].strip()\n",
    "        rejected = example[\"rejected\"].split(sep)[1].strip()\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": chosen,\n",
    "            \"rejected\": rejected,\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"prompt\": \"\",\n",
    "            \"chosen\": \"\",\n",
    "            \"rejected\": \"\"\n",
    "        }\n",
    "\n",
    "def preprocess_dataset(split):\n",
    "    dataset = load_dataset(\"Anthropic/hh-rlhf\", split=split)\n",
    "    dataset = dataset.map(split_prompt_response)\n",
    "    dataset = dataset.filter(\n",
    "        lambda x: len(x[\"prompt\"] + x[\"chosen\"]) <= args.max_length\n",
    "                  and len(x[\"prompt\"] + x[\"rejected\"]) <= args.max_length\n",
    "                  and x[\"prompt\"] != \"\"\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Use small subsets for faster experimentation\n",
    "train_dataset = preprocess_dataset(\"train[:1%]\")\n",
    "eval_dataset = preprocess_dataset(\"train[1%:2%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a4ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Setup DPO Config\n",
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "    max_steps=args.max_steps,\n",
    "    logging_steps=args.logging_steps,\n",
    "    save_steps=args.save_steps,\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    gradient_checkpointing=args.gradient_checkpointing,\n",
    "    learning_rate=args.learning_rate,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=args.eval_steps,\n",
    "    output_dir=args.output_dir,\n",
    "    beta=args.beta,\n",
    "    report_to=args.report_to,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=10,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    bf16=torch_dtype == torch.bfloat16,\n",
    "    remove_unused_columns=False,\n",
    "    run_name=\"dpo_llama_colab\",\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=args.lora_r,\n",
    "    lora_alpha=args.lora_alpha,\n",
    "    lora_dropout=args.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904798dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train using DPOTrainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train and Save\n",
    "dpo_trainer.train()\n",
    "dpo_trainer.save_model(os.path.join(args.output_dir, \"final_checkpoint\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
