
# ðŸ§  Reinforcement Learning from Human Feedback (RLHF)

This repository contains experimental and production-ready implementations of **Reinforcement Learning from Human Feedback (RLHF)** techniques, focusing on:
- âœ… Direct Preference Optimization (DPO)
- âœ… Proximal Policy Optimization (PPO)
- âœ… Sentiment-Aligned Language Model Fine-Tuning
- âœ… TRL + HuggingFace-based RLHF pipelines



##  ðŸ“‚ Folder Structure


```bash
RLHF/
â”‚
â”œâ”€â”€ DPO/             # DPO fine-tuning (Anthropic HH-RLHF or custom datasets)
â”œâ”€â”€ PPO/             # PPO fine-tuning using reward models (IMDb, sentiment, etc.)
â”œâ”€â”€ README.md        # Project overview and instructions
â””â”€â”€ ...
```

## Setup

# 1. Clone the repository
git clone https://github.com/09-prince/RLHF.git

cd RLHF/DPO

cd RLHF/PPO

# 2. Install dependencies
pip install -r requirements.txt


## Authors

- Created by [09-prince](https://github.com/09-prince)
## ðŸ“š Blog

- ðŸ”¹ [**Reinforcement Learning from Human Feedback â€“ DPO Edition**](https://medium.com/@gourprince2004/reinforcement-learning-from-human-feedback-7dc0dc32fd4e)  
  Dive into how Direct Preference Optimization (DPO) is used to align large language models using pairwise human preferences instead of reward scores.

<!-- - ðŸ”¸ [Understanding PPO (Proximal Policy Optimization)](https://your-blog-link.com/ppo-post) *(Coming Soon)*  
  A step-by-step guide to PPO and how it fits into the RLHF pipeline. -->
